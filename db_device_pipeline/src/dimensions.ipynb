{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee353e42-ff58-4955-9608-12865bd0950e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Get device Dimension Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30aad326-fb81-4ada-9023-b84488fc1b99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install geopy Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f78f0e87-b622-464f-9d32-a48e7cef1589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql import udf\n",
    "\n",
    "# Sample UK cities and reservoir names\n",
    "uk_cities = [\"London\", \"Manchester\", \"Birmingham\", \"Leeds\", \"Glasgow\", \"Southampton\", \"Liverpool\", \"Newcastle\", \"Nottingham\", \"Sheffield\"]\n",
    "reservoirs = [\"Kielder Water\", \"Rutland Water\", \"Lake Vyrnwy\", \"Derwent Reservoir\", \"Pitsford Water\", \"Bewl Water\", \"Grafham Water\", \"Carsington Water\", \"Chew Valley Lake\", \"Ladybower Reservoir\"]\n",
    "\n",
    "# Generate dummy data\n",
    "data = [Row(device_id=i+1, \n",
    "            location=uk_cities[i], \n",
    "            site=reservoirs[i], \n",
    "            site_manager_email=f\"manager{i}@example.com\") for i in range(10)]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "pdf = df.toPandas()\n",
    "\n",
    "\n",
    "# Display the DataFrame\n",
    "display(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9dc7172-de1f-4a63-a003-547fcf1c4305",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "\n",
    "def get_long_lat(s):\n",
    "  geolocator = Nominatim(user_agent=\"geo_lookup\")\n",
    "  country =\"UK\"\n",
    "  loc = geolocator.geocode(s['location']+','+ country)\n",
    "  s['longitude'] = loc.longitude\n",
    "  s['latitude'] = loc.latitude\n",
    "  return s\n",
    "\n",
    "\n",
    "pdf1 = pdf.apply(get_long_lat, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f585142-ef68-4418-8c4e-a4c8413e00aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame(pdf1)\n",
    "df1.write.mode('overwrite').option('overwriteSchema', True).saveAsTable('alex_young.device_demo.device_dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e566f27-2b0d-4b6c-ada1-258ba7de50a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8ff342-ce4f-459b-86c9-401dae9ebffb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as SF\n",
    "\n",
    "\n",
    "\n",
    "df = (spark.read.table('alex_young.device_demo.device_enriched')\n",
    "      .filter(SF.col('attribute')==SF.lit('Flow'))\n",
    "\n",
    "      )\n",
    "                  \n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "abed5396-ba78-48fc-825a-fb3eec03762b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b281dc25-4be3-4568-b0a1-3f747b6bab3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "predict_udf = mlflow.pyfunc.spark_udf(spark, model_uri=f\"models:/alex_young.device_demo.device_error_voltage_and_flow@main\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78ed2d09-4c00-45fd-a12a-e5286ae4a96d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create the PySpark UDF\n",
    "\n",
    "\n",
    "input_cols=['attribute','value']\n",
    "\n",
    "df1 = (df\n",
    "       .withColumn('predicted_error', predict_udf(SF.struct(input_cols)))\n",
    "       .withColumn('predicted_error', SF.col('predicted_error').getItem(0))\n",
    "      )\n",
    "\n",
    "display(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2fe24d3-0ebb-41d9-9318-e8c0c32ad2d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "@dlt.table(\n",
    "  comment=\"DLT for predictions scored by  lr_model_cameronshotwell model based on aia_my_prdcld.config_memory_evolution Delta table.\",\n",
    "  table_properties={\n",
    "    \"quality\": \"gold\"\n",
    "  }\n",
    ")\n",
    "def  lr_model_cameronshotwell_predictions():\n",
    "  return (\n",
    "    dlt.read(input_dlt_table_name)\n",
    "    .withColumn('prediction', predict(struct(input_dlt_table_columns)))\n",
    "  )\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2683910998003499,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "dimensions",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
