{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8657d45-183d-4f46-adf3-ec089394fa89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install mlflow cloudpickle databricks-automl-runtime holidays lz4 psutil category-encoders scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9198e987-5606-403d-9f6d-8f14e6a4017f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import DLT and src/db_device_pipeline\n",
    "import dlt\n",
    "# import sys\n",
    "# sys.path.append(spark.conf.get(\"bundle.sourcePath\", \".\"))\n",
    "# from pyspark.sql.functions import expr\n",
    "# from db_device_pipeline import main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4703ee75-d54e-4f5f-82ef-eacf8ceeabd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "  volume_location = spark.conf.get(\"pipeline.volumeLocation\")\n",
    "except Exception as e:\n",
    "  volume_location = None\n",
    "if volume_location is None or volume_location == \"\":\n",
    "  volume_location = \"/Volumes/alex_young/device_demo\"\n",
    "\n",
    "print(volume_location)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1a348d6-42b9-46e1-bc8b-710780ed65c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType, TimestampType\n",
    "from pyspark.sql import functions as SF\n",
    "\n",
    "@dlt.expect(\"time_greater_than_2024\", \"time > '2024-01-01'\")\n",
    "@dlt.expect(\"expect_value_less_that_55\", \"value <= 55\")\n",
    "@dlt.expect_or_drop(\"null device_id\", \"device_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"null attribute\", \"attribute IS NOT NULL\")\n",
    "@dlt.expect_or_drop('null value', \"value IS NOT NULL\")\n",
    "@dlt.table(\n",
    "  name=\"alex_young.device_demo.raw_device_data\",\n",
    "  comment=\"raw table for device data\",\n",
    "  schema = StructType([\n",
    "    StructField('attribute', StringType(), False),\n",
    "    StructField('device_id', LongType(), False),\n",
    "    StructField('endpoint_id', LongType(), False),\n",
    "    StructField('time', TimestampType(), True),\n",
    "    StructField('unixtime', LongType(), True),\n",
    "    StructField('value', DoubleType(), False),\n",
    "    StructField('_rescued_data', StringType(), True)\n",
    "  ]),\n",
    "  cluster_by=['device_id', 'time']\n",
    ")\n",
    "def get_raw_device_data():\n",
    "  raw_data = (spark.readStream\n",
    "    .format(\"cloudFiles\")      \n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option('multiline', 'true')\n",
    "    .load(volume_location)\n",
    "    .withColumn(\"time\", SF.col(\"time\").cast(\"timestamp\"))\n",
    "    .withColumn(\"device_id\", SF.col(\"device_id\").cast(\"long\"))\n",
    "    .withColumn(\"endpoint_id\", SF.col(\"endpoint_id\").cast(\"long\"))\n",
    "    .withColumn(\"value\", SF.col(\"value\").cast(\"double\"))\n",
    "    .withColumn(\"unixtime\", SF.col(\"unixtime\").cast(\"long\"))\n",
    "\n",
    "  )\n",
    "  return raw_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "696ef680-ac5d-451d-ab18-ec142575e4f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.view(\n",
    "  name=\"device_dimensions_view\",\n",
    "  comment=\"raw view on top of device data\",\n",
    ")\n",
    "def get_device_dimensions_view():\n",
    "  return (spark.read.table('alex_young.device_demo.device_dimensions'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6dc638b-6ff0-41b8-87af-f0cdc420fdbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import mlflow.pyfunc\n",
    "predict_udf = mlflow.pyfunc.spark_udf(spark, model_uri=f\"models:/alex_young.device_demo.device_error_voltage_and_flow@main\",env_manager=\"local\")\n",
    "input_cols=['attribute','value']\n",
    "\n",
    "@dlt.table(\n",
    "  name=\"alex_young.device_demo.device_enriched\",\n",
    "  comment='dimension_data_added_to_device_stream',\n",
    "  cluster_by=['device_id', 'time']\n",
    ")\n",
    "def get_device_combined():\n",
    "  df_dim = spark.read.table('device_dimensions_view')\n",
    "  df_raw = spark.readStream.table('alex_young.device_demo.raw_device_data')\n",
    "  df_raw_scored = (\n",
    "    df_raw.drop('endpoint_id')\n",
    "    .withColumn('predicted_error', predict_udf(SF.struct(input_cols)))\n",
    "    .withColumn('predicted_error', SF.col('predicted_error').getItem(0))\n",
    "  )\n",
    "\n",
    "  return (df_raw_scored.join(df_dim, on='device_id', how='left'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f37a9a2-bb36-4e7f-905f-b9dfc652c079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.view(\n",
    "  name=\"device_flow\",\n",
    "  comment=\"raw view on top of device data\",\n",
    ")\n",
    "def get_device_flows():\n",
    "  return  (spark.read.table('alex_young.device_demo.device_enriched')\n",
    "      .filter(SF.col('attribute')==SF.lit('Flow'))\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd6bdafe-610f-4971-aca3-a4e834a5f494",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.view(\n",
    "  name=\"device_voltage\",\n",
    "  comment=\"raw view on top of device data\",\n",
    ")\n",
    "def get_device_voltage():\n",
    "  return  (spark.read.table('alex_young.device_demo.device_enriched')\n",
    "      .filter(SF.col('attribute')==SF.lit('Voltage'))\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7db3f8e3-cef2-4ff6-b5be-b12c52aa16a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "@dlt.table(\n",
    "  name=\"alex_young.device_demo.device_agg_by_time\",\n",
    "  comment='aggregate_table_avg_value_by_10_mins',\n",
    "  cluster_by=['device_id','start_datetime']\n",
    ")\n",
    "def get_device_aggregates():\n",
    "  return (\n",
    "    spark.readStream.table(\"alex_young.device_demo.device_enriched\")\n",
    "      .select('device_id', 'time','attribute', 'value', 'location', 'site','preditected_error','longitude','latitude')\n",
    "      .withWatermark(\"time\", \"1 hour\")\n",
    "      .groupBy(SF.col('device_id'), SF.col('attribute'), SF.window(\"time\", \"10 minutes\").alias(\"time\"))\n",
    "      .agg(SF.avg('value').alias('avg_value'),\n",
    "           SF.min('value').alias('min_value'),\n",
    "           SF.max('value').alias('max_value'),\n",
    "           SF.first('location').alias('location'),\n",
    "           SF.first('longitude').alias('longitude'),\n",
    "           SF.first('latitude').alias('latitude'),\n",
    "           SF.first('site').alias('site'),\n",
    "           SF.sum('predicted_error').alias('preditected_erros')\n",
    "      )\n",
    "      .withColumn('start_datetime', SF.col('time.start'))\n",
    "      .withColumn('end_datetime', SF.col('time.end'))\n",
    "      .select('device_id','attribute', 'location', 'site','avg_value','min_value','max_value','start_datetime','end_datetime')\n",
    "\n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1",
    "dependencies": [
     "mlflow"
    ]
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2683910997928638,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "dlt_pipeline",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
